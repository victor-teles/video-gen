# ===================================================================
# Editur AI Backend - Production Environment Configuration
# ===================================================================
# Copy this file to .env and fill in your actual values
# Never commit .env to git - it contains sensitive information

# ===================================================================
# STORAGE CONFIGURATION
# ===================================================================
# Storage type: 's3' for production, 'local' for development only
STORAGE_TYPE=s3

# AWS S3 Configuration (REQUIRED for production)
S3_BUCKET_NAME=editur-ai-storage-YOUR_UNIQUE_ID
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
AWS_REGION=us-east-1

# ===================================================================
# DATABASE CONFIGURATION
# ===================================================================
# For small scale: SQLite (free, single file)
# For production scale: PostgreSQL (RDS or external)
DATABASE_URL=sqlite:///./editur.db
# DATABASE_URL=postgresql://username:password@host:port/database_name

# ===================================================================
# REDIS/CELERY CONFIGURATION
# ===================================================================
# Local Redis (free for small scale)
REDIS_URL=redis://localhost:6379/0
# For production: Use ElastiCache or external Redis
# REDIS_URL=redis://your-elasticache-endpoint:6379/0

CELERY_BROKER_URL=${REDIS_URL}
CELERY_RESULT_BACKEND=${REDIS_URL}

# ===================================================================
# API CONFIGURATION
# ===================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_TITLE=Editur AI Backend API
API_VERSION=1.0.0
API_DESCRIPTION=AI-powered video editing and faceless video generation

# CORS - Update with your frontend domain
ALLOWED_ORIGINS=https://editur.ai,http://localhost:3000,http://localhost:3001
RATE_LIMIT=100

# ===================================================================
# PROCESSING CONFIGURATION
# ===================================================================
# File upload limits
MAX_FILE_SIZE=524288000  # 500MB in bytes
DEFAULT_NUM_CLIPS=3
DEFAULT_RATIO=9:16
MAX_CLIPS_PER_REQUEST=5

# Video quality settings (higher CRF = more compression = smaller files)
VIDEO_CRF=23
AUDIO_BITRATE=128k

# Worker settings
CELERY_WORKER_CONCURRENCY=2
CELERY_LOG_LEVEL=info
CELERY_TASK_TIME_LIMIT=3600

# AI Model settings
WHISPER_MODEL_SIZE=base  # Options: tiny, base, small, medium, large
YOLO_MODEL=yolov8n.pt
AI_DEVICE=cpu  # cpu or cuda (if GPU available)

# ===================================================================
# AI SERVICES API KEYS (REQUIRED for faceless video generation)
# ===================================================================
# OpenAI API Key (for GPT-4 and TTS)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1

# Replicate API Token (for image generation)
# Get from: https://replicate.com/account/api-tokens
REPLICATE_API_TOKEN=r8_your_replicate_token_here

# Optional: FAL Key (alternative to Replicate)
# FAL_KEY=your_fal_key_here

# ===================================================================
# FACELESS VIDEO GENERATION SETTINGS
# ===================================================================
# Story generation
STORY_MODEL=gpt-4o-mini  # Cheaper alternative: gpt-3.5-turbo
STORY_TEMPERATURE=0.9
STORY_CHAR_LIMIT_MIN=700
STORY_CHAR_LIMIT_MAX=800
MAX_SCENES=12

# Image generation
IMAGE_MODEL=black-forest-labs/flux-schnell  # Fast and cheaper
IMAGE_ASPECT_RATIO=9:16
IMAGE_INFERENCE_STEPS=4
IMAGE_GUIDANCE=3.0
IMAGE_QUALITY=95

# Text-to-speech
TTS_MODEL=tts-1  # tts-1 is cheaper than tts-1-hd
TTS_SPEECH_RATE=1.1

# Cost control limits
MAX_FACELESS_VIDEOS_PER_USER=10  # Per day
MAX_STORY_TITLE_LENGTH=100
MAX_STORY_DESCRIPTION_LENGTH=500
MAX_STORY_CONTENT_LENGTH=2000

# Cost estimation (for monitoring)
OPENAI_COST_PER_TOKEN=0.00003
OPENAI_TTS_COST_PER_CHAR=0.000015
REPLICATE_COST_PER_IMAGE=0.05

# ===================================================================
# SECURITY & MONITORING
# ===================================================================
# Debug mode (NEVER set to true in production)
DEBUG=false
AUTO_RELOAD=false
SHOW_ERROR_DETAILS=false

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/editur-ai/app.log

# File cleanup (hours)
CLEANUP_UPLOADS_AFTER=24
CLEANUP_RESULTS_AFTER=72

# ===================================================================
# OPTIONAL: MONITORING & NOTIFICATIONS
# ===================================================================
# Webhook URL for notifications (spot interruption, errors, etc.)
# WEBHOOK_URL=https://hooks.slack.com/your/webhook/url

# Sentry for error tracking (recommended for production)
# SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# ===================================================================
# DEVELOPMENT/TESTING OVERRIDES
# ===================================================================
# Uncomment these for local development
# STORAGE_TYPE=local
# DATABASE_URL=sqlite:///./dev.db
# DEBUG=true
# SHOW_ERROR_DETAILS=true
# MAX_FILE_SIZE=52428800  # 50MB for testing

# ===================================================================
# COST OPTIMIZATION NOTES
# ===================================================================
# 1. Use smaller AI models for lower costs:
#    - WHISPER_MODEL_SIZE=tiny (fastest, good quality)
#    - STORY_MODEL=gpt-3.5-turbo (90% cheaper than GPT-4)
#    - TTS_MODEL=tts-1 (cheaper than tts-1-hd)
#
# 2. Set strict limits:
#    - Lower MAX_FACELESS_VIDEOS_PER_USER
#    - Higher VIDEO_CRF for smaller files
#    - Lower MAX_FILE_SIZE
#
# 3. Monitor usage:
#    - Check AWS CloudWatch for costs
#    - Set up billing alerts
#    - Use AWS Cost Explorer
#
# 4. S3 cost optimization:
#    - Enable lifecycle policies to delete old files
#    - Use S3 Intelligent Tiering
#    - Compress uploads before storage